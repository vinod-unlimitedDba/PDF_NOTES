
Overview of the Chapter
The chapter provides a perspective on the role of relational databases in software development and its performance issues during software maintenance. During the 1970s 
two major innovations appeared in the software industry:

• Introduction of relational databases as an improvement over the files and nonrelational databases, both of which were cumbersome to use, and
• Structured methodology that later became known as the software development life cycle process.

E.F. Codd was the inventor of the former and several contributors such as Yourdon
and Constantine, Gane and Sarson, and Jackson formulated the latter. Database development is an integral part of the software development life cycle (SDLC) and software
maintenance life cycle (SMLC). It consists of five phases, the first four of which encompass SDLC and the last one occurs under SMLC. 


The first commercially available
relational database management system (RDBMS) was INGRES, marketed in 1979 by
Relational Technology, Inc. This was followed shortly by ORACLE, marketed in the
same year by Relational Software, Inc., which was later renamed Oracle Corporation.



Software Development Life Cycle (SDLC)
Under this methodology an application development consists of five distinct phases, as described below (see [9]):
(a) Problem definition and feasibility study,
(b) Requirements analysis,
(c) Preliminary system design,
(d) Detailed system design, and
(e) System implementation, maintenance, and evaluation.


These five phases are grouped into three logical categories: 
analysis consisting of (a)and (b), 
design consisting of (c) and (d), and 
implementation consisting of (e) alone.

These five phases are grouped into three logical categories: analysis consisting of (a)
and (b), design consisting of (c) and (d), and implementation consisting of (e) alone.

The analysis category is guided by the keyword WHAT, i.e., what functions will the
application provide for the end users. Its end product consists of a logical system specification (also called functional requirements document) 
that describes in detail the tasks that end users will be able to do when the system becomes operational.


Various technologies have appeared since then such as object-oriented design and programming, first two-tier and then
n-tier (n > 2) client server architecture, Web-based design, event-driven programming instead of logic-driven programming, etc.

The main differences between SDLC and SMLC arise from the focus: SMLC operates with an existing application, whereas SDLC builds a new application. The salient
points of difference are listed below.
• An SMLC job is initiated by a software maintenance request form, whereas an
SDLC job starts as a new application development.
• SMLC deals with a code and a database that are already in existence and need modification, whereas SDLC involves a new code and database.
• The analysis and design phases of SDLC are much longer and require in-depth discussion with the end users, whereas they are much simpler in SMLC.



The analysis and design phases of the SDLC and SMLC result in the following database tasks,
(a) Enterprise modeling,
(b) Logical database design,
(c) Physical database design,
(d) Database implementation, and
(e) Database maintenance.


The starting point of any data modeling is a pool of data elements that are collected during the analysis phase of the SDLC. They are gathered from source documents such as
data entry forms, data entry screen captures, and reports, and also during the interviews
with the end users as a part of the requirements collection process.


Step 1: Identify as much as possible the primitive data elements and decompose the
group level data elements into their component primitive data elements.

1.3.1 Entity, Relationship, and Attribute
The next task is to bring order into this data pool chaos by grouping the data into separate
sets such that each set consists of data elements representing an atomic concept such as
person, place, object, event, etc


A key is defined as a set of attribute(s) in an entity or a relationship such that given a value of the key the search returns either no record or only one record. When multiple sets of attribute(s) qualify for a
key, each set is called a candidate key. One of them is designated a primary key and the
rest are called alternate key(s).

Step 2: Create distinct sets of data elements from the pool such that each set represents
an atomic concept.

Step 3: Create additional sets, as needed, so that distinct atomic sets are linked by
them. Such link sets do not represent atomic concepts. Instead, they relate two
or more atomic concepts

Step 4: Determine how different entities are related and replace each M:N type of relationship with two distinct relationships each of type 1:N. As a minimum,
each relationship contains the primary keys of both linked entities as attributes.

Step 5: Break down each vector valued attribute into multiple records, each containing
a single value from the vector and repeat the remaining attributes.


Step 6: Identify entities that can be classified as supertypes and subtypes and separate
them into distinct entities such that each subtype is a supertype and each supertype is one of the subtypes.


The three normal forms are described below with examples.
First Normal Form (1NF)—A Table Is in 1NF if It Satisfies the Following
Four Conditions
• There are no vector valued attributes.
• There are no duplicate rows.
• All values of a column belong to the domain of that column.
• The order of rows and columns is immaterial.
• All values of a column belong to the domain of that column.
• The order of rows and columns is immaterial.


Second Normal Form (2NF)—A Table Is in 2NF if It Satisfies the Following Two Conditions
• It is in 1NF.
• Exactly one of the following three possibilities holds:
—PK consists of a single column,
—PK consists of all the columns, or
—every non-PK column depends on the entire PK.


Second Normal Form (2NF)—A Table Is in 2NF if It Satisfies the Following Two Conditions
• It is in 1NF.
• Exactly one of the following three possibilities holds:
—PK consists of a single column,
—PK consists of all the columns, or
—every non-PK column depends on the entire PK



Third Normal Form (3NF)—A Table Is in 3NF if It Satisfies the Following
Two Conditions:
• It is in 2NF.
• It has no transitive dependency.
As an example, each table in Figure 1.4 is in 3NF.


A table has transitive dependency (TD) if a non-PK column functionally depends on
another non-PK column without involving the PK. TD is undesirable because, theoretically,
a table with TD can cause update anomalies by incurring loss of information.

The five important tasks of this phase are as follows.
(a) Ensure that each table is in the third normal form (3NF). For performance reasons it
may be necessary later to do some denormalization. But the logical database should
start with all 3NF tables.
(b) Assign to each column in each table a data type and a size according to the specifications of the RDBMS that will be used later during the physical database design
phase.
(c) Formulate all data validations that can be enforced in the next phase through declarative constraints, i.e., clauses such as REFERENCES, CHECK, etc. of the data
definition language (DDL) commands for creating or altering tables. REFERENCES
involves the principle of referential integrity discussed below in Section 1.4.3.
CHECK ensures the entry of a discrete or continuous range of valid values for column(s).
(d) Formulate additional data validation algorithms that cannot be enforced through declarative constraints. These are implemented through triggers, procedures, and functions using proprietory procedural languages such as PL/SQL in Oracle or through
host language interface programs using embedded SQL such as Pro*C,
Pro*COBOL, etc. in Oracle.
(e) Formulate additional indexing requirements besides the declaration of primary keys,
if known at this point in time.The five important tasks of this phase are as follows.
(a) Ensure that each table is in the third normal form (3NF). For performance reasons it
may be necessary later to do some denormalization. But the logical database should
start with all 3NF tables.
(b) Assign to each column in each table a data type and a size according to the specifications of the RDBMS that will be used later during the physical database design
phase.
(c) Formulate all data validations that can be enforced in the next phase through declarative constraints, i.e., clauses such as REFERENCES, CHECK, etc. of the data
definition language (DDL) commands for creating or altering tables. REFERENCES
involves the principle of referential integrity discussed below in Section 1.4.3.
CHECK ensures the entry of a discrete or continuous range of valid values for column(s).
(d) Formulate additional data validation algorithms that cannot be enforced through declarative constraints. These are implemented through triggers, procedures, and functions using proprietory procedural languages such as PL/SQL in Oracle or through
host language interface programs using embedded SQL such as Pro*C,
Pro*COBOL, etc. in Oracle.
(e) Formulate additional indexing requirements besides the declaration of primary keys,
if known at this point in time.


A data dictionary provides a central
repository to document the work done during Phases (1) and (2), and is often described as
a database about a database, or a meta database. 

A CASE(Computer Aided Software Engineering) tool such as ERwin, Power Designer,
Oracle Designer, etc. although some desktop RDBMS software such as Microsoft Access tool automatically builds the data dictionary as the enterprise data model is
created. 


a. COLUMN (Column Name, Column Description, Column Format, Data
Source, Load Procedure, Validation, Comment)
PK = Column Name.


Oracle partitions the physical disk space of a database into a hierarchy of components: tablespace, segment, extent, and block. A database consists of multiple tablespaces, which are logical concepts. Each tablespace spans one or more physical disk files
that are explicitly assigned to it.

A tablespace consists of many segments such as table,index, temporary, rollback, etc. 

Thus, a segment is a logical concept and is mapped onto multiple chunks of physical 
storage areas called extents. 

The extents belonging to a segment need not be contiguous.

 Each extent, however, consists of multiple contiguous
blocks, which are the lowest indivisible units of disk space.


A segment consists of an initial extent and many next extents
with the total number of extents not exceeding a parameter called maxextent.


Block Size Maxextent
2K 121
4K 249
8K 505
16K 1017

**********************************************
2                                            *
Performance Tuning Methodology               *
***********************************************

Three Levels of a Database
=====================
The architecture of a database consists of three distinct but interrelated levels: conceptual,
internal, and external. 

The conceptual level involves a representation of the entire information content of the
database and, as such, is the most important in a database application. The conceptual
level is defined here to include ALL structural information about all database objects be
longing to the database.

The internal level, also called the physical level, is the one closest to the physical
storage, where the data contents of the conceptual level are stored on disk files. In addition, the internal level contains a set of memory resident data structures that Oracle calls the System Global Area, and a set of background processes. 

The external level is closest to the users. It is concerned with the way in which the users view the data for their own use. Thus, different users access different external levels
of the database. The user can be an end user, a database developer, or a DBA working on
some specific query generation. Each external level consists of a subset of the conceptual
level.



In an n-tier (n ≥ 3) client-server architecture the GUI type
external levels normally reside on the application tier(s) comprising levels 2 through n –
1. They access the database residing on the database server at the nth tier through the intervention of application programs. But the external level of any user accessing the database through the command line interface resides on the database server at the nth tier



Fragmentation occurs when the extents of a table are not contiguous, or when the free space of a tablespace consists of many small sets of noncontiguous extents instead of a few large sets of contiguous extents. It degrades database
performance because a single table with many noncontiguous extents is scattered over
many small isolated fragments.


Chaining and migration of rows (see Section 5.9) in a table can occur due to inserts
and updates of records when its PCTFREE parameter is set incorrectly or when columns
with VARCHAR2 data types that were initially NULL start getting updated with nonnull
values

Tables with LONG or LONG RAW data types deserve special attention since
they almost always cause chaining.


The following is a preliminary checklist of tuning activities at each level.

Conceptual Level
• Denormalization
• Partitioning of Very Large Tables
• Data Replication in Distributed Database
Internal Level
• Data Storage
Two main reasons for inefficiency of data storage are fragmentation of tablespaces, and
chaining and migration of rows in tables.
• Memory Usage
Here the goal is to make data available in memory instead of retrieving them from the
disks since the data transfer rate for memory is an order of magnitude faster than that
from auxiliary storage.


If the data load with indices takes longer than
10 minutes or so, then I drop the indices first, load the data into the table, and then create
the indices for the updated table.

SYSTEMSTATE Dump 
SYSTEMSTATE dumps are performed using nonread consistent techniques, meaning that the data output 
at the end of a long duration dump may not have existed in its current state when the dump was 
initiated.




 Performance Issues at the Conceptual Level
As indicated in Section 2.2, performance issues manifest themselves in the form of high
response time that is unacceptable to the users. During the development phase the developers and the DBAs jointly address such issues by tuning all three levels of the database.
The improved response time is then demonstrated during the test phase. The conceptual
level can be tuned through these activities:
(a) Denormalization of the schema to reduce the number of joins in ad hoc queries and
reports;
(b) Creation of new indices or modification of existing indices to force indexed search of
tables instead of full table scans, where appropriate;
(c) Integration of views into queries to avoid long-running SELECT statements involving views;
(d) Partitioning of very large tables to take advantage of parallel full table scans or to access only some partitions instead of accessing the entire tables;
(e) Data replication for copying the same conceptual level at multiple locations to reduce query processing time and to avoid failed updates when communication links to
some of the remote sites are down. Each of the above items involves a change in the
conceptual level, which, therefore, must be updated with each change. If ERwin or
any other similar CASE tool is used, such changes are automatically migrated to the
relevant scripts for schema creation.


On Line Transaction Processing (OLTP) applications update tables through online transactions performed by the users.
